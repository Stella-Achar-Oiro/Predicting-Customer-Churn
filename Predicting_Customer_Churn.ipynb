{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4JR2mn4oWZX2huwg3z3qZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stella-Achar-Oiro/Predicting-Customer-Churn/blob/main/Predicting_Customer_Churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stella Achar Oiro\n",
        "### Follow Me on Social Media:\n",
        "\n",
        "- [Twitter](https://twitter.com/Stella_Oiro)\n",
        "- [GitHub](https://github.com/Stella-Achar-Oiro)\n",
        "- [LinkedIn](https://www.linkedin.com/in/stella-achar-oiro/)\n"
      ],
      "metadata": {
        "id": "D0KNYqvIpGuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting customer churn is a crucial task for telecom companies like Sprint, as retaining customers is often more cost-effective than acquiring new ones. To create a machine learning model that predicts customer churn, you can follow these steps:"
      ],
      "metadata": {
        "id": "Hf0YtCzLo7fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1\n",
        "**Data Collection and Preprocessing:**\n",
        "\n",
        "**Gather historical data:** Collect relevant data on customer behavior, demographics, purchase history, customer service interactions, and past churn instances.\n",
        "\n",
        "\n",
        "**Data cleaning:** Clean the dataset by handling missing values, outliers, and inconsistencies.\n",
        "\n",
        "\n",
        "**Feature engineering:** Create new features or transform existing ones that might be informative for predicting churn, such as customer tenure, usage patterns, and customer feedback scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZJ2Fkzvc8IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('customer_churn_data.csv')\n",
        "\n",
        "# Data cleaning\n",
        "data.dropna(inplace=True)  # Handle missing values\n",
        "data = data[~data['TotalCharges'].str.isspace()]  # Remove rows with empty TotalCharges\n",
        "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])  # Convert TotalCharges to numeric\n"
      ],
      "metadata": {
        "id": "2WVevaMDdE_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2\n",
        "**Exploratory Data Analysis (EDA):**\n",
        "\n",
        "**Analyze the dataset:** Conduct exploratory data analysis to gain insights into customer behavior, identify correlations, and understand patterns related to churn.\n",
        "\n",
        "\n",
        "**Visualization:** Visualize data using charts and graphs to better understand relationships between variables."
      ],
      "metadata": {
        "id": "HKLz-c4CdHrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA - Get summary statistics\n",
        "summary_stats = data.describe()\n",
        "\n",
        "# Visualize churn distribution\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.countplot(x='Churn', data=data)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dJUphY6EdXOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3\n",
        "**Data Splitting:**\n",
        "\n",
        "Split the data into training and testing sets to assess the model's performance."
      ],
      "metadata": {
        "id": "nJJGzh-qdYC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and target variable\n",
        "X = data.drop('Churn', axis=1)\n",
        "y = data['Churn']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "o7eJv1bmdpfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4\n",
        "**Feature Selection:**\n",
        "\n",
        "Identify and select the most relevant features that have the most significant impact on churn prediction.\n",
        "Use feature importance from tree-based models, correlation analysis, or dimensionality reduction methods."
      ],
      "metadata": {
        "id": "EXR68FmzdqI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection (example: using correlation)\n",
        "correlation_matrix = data.corr()\n",
        "relevant_features = correlation_matrix['Churn'].sort_values(ascending=False).index[:10]\n",
        "X_train = X_train[relevant_features]\n",
        "X_test = X_test[relevant_features]\n"
      ],
      "metadata": {
        "id": "SU99c6pqdvBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5\n",
        "**Model Selection:**\n",
        "\n",
        "Choose appropriate machine learning algorithms for classification tasks. Common choices include Logistic Regression, Random Forest, Gradient Boosting, Support Vector Machines, and Neural Networks.\n",
        "Experiment with multiple algorithms and evaluate their performance."
      ],
      "metadata": {
        "id": "UfHLY3XodvqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose and initialize a machine learning algorithm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n"
      ],
      "metadata": {
        "id": "zw-LLpruh07a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6\n",
        "**Model Training:**\n",
        "\n",
        "Train the selected models using the training dataset. Tune hyperparameters for better model performance through techniques like grid search or random search."
      ],
      "metadata": {
        "id": "G3OnFkOqh1uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "rjPyk0HYiDtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7\n",
        "**Model Evaluation:**\n",
        "\n",
        "Evaluate the models' performance on the testing dataset using relevant metrics such as accuracy, precision, recall, F1-score, and ROC AUC.\n",
        "Consider the business context and the cost of false positives and false negatives when selecting evaluation metrics."
      ],
      "metadata": {
        "id": "MkpFHr6PiEZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}\\nClassification Report:\\n{report}')\n"
      ],
      "metadata": {
        "id": "DLit1JZCiSYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8\n",
        "Interpret the model's predictions to understand which features are driving churn predictions to understand the factors influencing customer decisions."
      ],
      "metadata": {
        "id": "dnaM5ynEiTAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importances\n",
        "feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "id": "uQIZMd-Aim0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9\n",
        "Deploy the chosen model in a production environment where it can generate predictions for new customer data.\n",
        "Implement automation for regular model retraining to keep it up to date."
      ],
      "metadata": {
        "id": "oQUtMW6mink2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Deploy the chosen machine learning model using Docker and FastAPI\n",
        "### Create FastAPI App\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "\n",
        "# Load the trained model (replace 'model.pkl' with your actual model file)\n",
        "model = joblib.load('model.pkl')\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class Item(BaseModel):\n",
        "    feature1: float\n",
        "    feature2: float\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(item: Item):\n",
        "    # Prepare the input features for prediction\n",
        "    input_features = [item.feature1, item.feature2]\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    prediction = model.predict([input_features])\n",
        "\n",
        "    return {\"prediction\": prediction[0]}\n"
      ],
      "metadata": {
        "id": "Y5O4MLLTi02n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a Dockerfile in the same directory as FastAPI app\n",
        "# Use the official Python image\n",
        "FROM python:3.9\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the requirements file into the container and install dependencies\n",
        "COPY requirements.txt requirements.txt\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "# Copy your FastAPI app script and model file into the container\n",
        "COPY app.py app.py\n",
        "COPY model.pkl model.pkl\n",
        "\n",
        "# Expose port 8000 (or the port your FastAPI app is running on)\n",
        "EXPOSE 8000\n",
        "\n",
        "# Start the FastAPI app when the container is run\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
      ],
      "metadata": {
        "id": "V0RvZEI9mxVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Create a Requirements File\n",
        "### Create a requirements.txt file listing the Python dependencies required for the FastAPI app.\n",
        "fastapi==0.68.1\n",
        "uvicorn==0.15.0\n",
        "joblib==1.0.1\n"
      ],
      "metadata": {
        "id": "YpQtPz8cnMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Build the Docker Image\n",
        "### Open a terminal and navigate to the directory containing your Dockerfile, app.py, model.pkl, and requirements.txt files.\n",
        "docker build -t my-fastapi-model .\n"
      ],
      "metadata": {
        "id": "BXl1n6Zbnk11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Run the Docker Container\n",
        "docker run -d -p 8000:8000 my-fastapi-model\n"
      ],
      "metadata": {
        "id": "9JHk9jRooKcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Access the FastAPI App at http://localhost:8000/predict/\n"
      ],
      "metadata": {
        "id": "HyeAIgMSoU5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10\n",
        "Continuously monitor the model's performance in real-world scenarios.\n",
        "Collect feedback on false positives and false negatives from customer service teams or other relevant sources to improve the model over time."
      ],
      "metadata": {
        "id": "3cqNmBRMi1e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11\n",
        "Utilize the model's predictions to proactively identify high-risk customers.\n",
        "Develop targeted retention strategies, such as personalized offers, discounts, or improved customer service, to reduce churn."
      ],
      "metadata": {
        "id": "HQCfhuZrjQh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12\n",
        "**A/B Testing:**\n",
        "\n",
        "Implement A/B tests to evaluate the effectiveness of retention strategies and refine them based on real-world results."
      ],
      "metadata": {
        "id": "kPdwcFF8jbiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13\n",
        "**Documentation and Reporting:**\n",
        "\n",
        "Maintain clear documentation of the entire model development process, including data preprocessing, feature engineering, and model training.\n",
        "Provide regular reports to stakeholders with insights on churn predictions and the impact of retention strategies."
      ],
      "metadata": {
        "id": "aqHE48BVjpDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By following these steps and iterating on my model and strategies, I can create a robust machine learning model for predicting customer churn and implement effective retention efforts to minimize customer loss for Sprint."
      ],
      "metadata": {
        "id": "tVnTk7BEj8HA"
      }
    }
  ]
}